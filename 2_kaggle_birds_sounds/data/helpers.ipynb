{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje Pomocnicze do Zadania\n",
    "\n",
    "Aby wszystkie funkcje zadziałały, notebook powinien być uruchamiany z następującym ułożeniem katalogów:\n",
    "\n",
    "```\n",
    "|- helpers.ipynb\n",
    "|- sampleSubmission.csv\n",
    "|- train\n",
    " |- {unzipped train files and labels}\n",
    "|- test\n",
    " |- {unzipped test files}\n",
    "```\n",
    "\n",
    "Należy odkomentować u siebie linijki zapisujące pliki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytywanie Danych\n",
    "\n",
    "Poniższe funkcje są przykładowymi funkcjami wczytującymi dane. Możliwe, że będą potrzebne modyfikacje reprezentacji i bardziej skomplikowana funkcja tworząca zbiór treningowy. Wczytywanie danych nie jest zaimplementowane optymalnie - między innymi kod wczytuje wielokrotnie ten sam plik. Dla bardziej złożonych reprezentacji możliwe, że trzeba będzie przepisać te funkcje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel(file_name, start=0, stop=None, n_mels=60):\n",
    "    '''Wczytuje mel spektrogram z pliku.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Nazwa pliku z nagraniem.\n",
    "        start (float): Sekunda, w której zaczyna się interesujący fragment.\n",
    "        stop (float): Sekunda, w której kończy się interesujący fragment.\n",
    "        n_mels (int): Liczba meli na spektrogramie (wysokość spektrogramu).\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Spektrogram.\n",
    "    '''\n",
    "    samples, sample_rate = librosa.core.load(file_name, sr = None)\n",
    "    samples = samples[int(start * sample_rate):int(stop * sample_rate) if stop else None]\n",
    "    spectrogram = librosa.feature.melspectrogram(y = samples, sr = sample_rate,\n",
    "                                                 n_mels = n_mels, fmin = 6000, fmax = 9000)\n",
    "#     spectrogram = stats.boxcox(spectrogram, lmbda=0.043, alpha=0.000001)\n",
    "\n",
    "#     mfccs = librosa.feature.mfcc(S=spectrogram, norm='ortho', dct_type=3)\n",
    "#     return mfccs\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "def load_spec(file_name, start=0, stop=None):\n",
    "    '''Wczytuje standardowy spektrogram z pliku.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Nazwa pliku z nagraniem.\n",
    "        start (float): Sekunda, w której zaczyna się interesujący fragment.\n",
    "        stop (float): Sekunda, w której kończy się interesujący fragment.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Spektrogram.\n",
    "    '''\n",
    "    sample_rate, samples = wavfile.read(file_name)\n",
    "    samples = samples[int(start * sample_rate):int(stop * sample_rate) if stop else None]\n",
    "    _, _, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "    return spectrogram\n",
    "\n",
    "def load_test(load_repr=load_mel):\n",
    "    '''Wczytuje dane testowe.\n",
    "    \n",
    "    Args:\n",
    "        load_repr (function): Funkcja wczytująca pożądaną reprezentację.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Tablica z danymi testowymi.\n",
    "    '''\n",
    "    with open('sampleSubmission.csv', 'r') as file:\n",
    "        lines = file.read().split()[1:]\n",
    "        sample_ids = [line.split(',')[0] for line in lines]\n",
    "        samples = np.array([s.split('/') for s in sample_ids])\n",
    "    \n",
    "    X_test = []\n",
    "    rec_files = [file_name for file_name in os.listdir('test') if file_name.endswith('.wav')]\n",
    "    for file_name in rec_files:\n",
    "        recording_id = file_name.split('.')[0][3:]\n",
    "        time_markers = samples[samples[:, 0] == recording_id, 1].astype(np.int)\n",
    "        for t in time_markers:\n",
    "            representation = load_repr(os.path.join('test', file_name), start = t, stop = t + 1)\n",
    "            X_test.append(representation)\n",
    "    return np.array(X_test)\n",
    "\n",
    "def load_test_2(load_repr=load_mel, interval=0.3, step=0.05):\n",
    "    '''Wczytuje dane testowe.\n",
    "    \n",
    "    Args:\n",
    "        load_repr (function): Funkcja wczytująca pożądaną reprezentację.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Tablica z danymi testowymi.\n",
    "    '''\n",
    "    compartment = interval / 2\n",
    "    with open('sampleSubmission.csv', 'r') as file:\n",
    "        lines = file.read().split()[1:]\n",
    "        sample_ids = [line.split(',')[0] for line in lines]\n",
    "        samples = np.array([s.split('/') for s in sample_ids])\n",
    "    \n",
    "    X_test = []\n",
    "    rec_files = [file_name for file_name in os.listdir('test') if file_name.endswith('.wav')]\n",
    "    recordings_ids = []\n",
    "    print('samples:', samples)\n",
    "    for file_name in tqdm(rec_files):\n",
    "        recording_id = file_name.split('.')[0][3:]\n",
    "        time_markers = samples[samples[:, 0] == recording_id, 1].astype(np.int)\n",
    "        for t in time_markers:\n",
    "            for i in np.arange(t + compartment, t + 1 - compartment, step): \n",
    "                representation = load_repr(os.path.join('test', file_name), start = i - compartment, stop = i + compartment)\n",
    "                X_test.append(representation)\n",
    "                recordings_ids.append(\"{}/{}\".format(recording_id, t))\n",
    "    return np.array(X_test), recordings_ids\n",
    "\n",
    "\n",
    "def read_labels():\n",
    "    '''Wczytuje etykiety czasowe z pliku labels.txt w folderze train.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: Tablica z etykietami czasowymi zawierająca kolumny: nr nagrania, sekunda początku dźwięku, sekunda końca dźwięku.\n",
    "    '''\n",
    "    labels = []\n",
    "    with open(os.path.join('train', 'labels.txt'), 'r') as file:\n",
    "        text = file.read()\n",
    "        for line in text.split('\\n')[1:]:\n",
    "            if len(line) > 1:\n",
    "                rec, start, stop = line.split(',')\n",
    "                rec, start, stop = int(rec[3:]), float(start), float(stop)\n",
    "                labels.append([rec, start, stop])\n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "def check_voices(second, labels, tol=0.):\n",
    "    '''Sprawdza czy w ramce czasowej [second, second+1] znajduje się głos według etykiet `labels`.\n",
    "    \n",
    "    Args:\n",
    "        second (float): Sekunda nagrania.\n",
    "        labels (ndarray): Tablica z etykietami, której 2 kolumna oznacza początek, a 3-cia - koniec nagrania.\n",
    "        tol (float): Tolerancja na brzegach fragmentu. Dźwięk, żeby był uznany, musi się kończyć po czasie `second+tol`\n",
    "            lub zaczynać przed czasem `second+1-tol`.\n",
    "    Returns:\n",
    "        bool: Czy w ramce czasowej jest odgłos ptaka.\n",
    "    '''\n",
    "    return (labels[1] >= second and labels[1] < second + 1 - tol) or \\\n",
    "           (labels[2] < second + 1 and labels[2] > second + tol) or \\\n",
    "           (labels[1] < second and labels[2] > second + 1)\n",
    "\n",
    "\n",
    "def map_seconds_to_y(labels):\n",
    "    '''Tworzy etykiety dla każdej kolejnej sekundy 10-sekundowego nagrania. -1 oznacza niepewną etykietę (urwane dźwięki na brzegach).\n",
    "    \n",
    "    Args:\n",
    "        labels (ndarray): Tablica z etykietami, której 2 kolumna oznacza początek, a 3-cia - koniec nagrania.\n",
    "    Returns:\n",
    "        ndarray: Tablica z binarnymi etykietami dla każdej z 10 sekund z możliwą niepewną etkietą -1.\n",
    "    '''\n",
    "    y = [0] * 10\n",
    "    y_restrictive = [0] * 10\n",
    "    for s in range(10):\n",
    "        for l in labels:\n",
    "            if check_voices(s, l):\n",
    "                y[s] = 1\n",
    "            if check_voices(s, l, 0.02):\n",
    "                y_restrictive[s] = 1\n",
    "        if y[s] != y_restrictive[s]:\n",
    "            y[s] = -1\n",
    "    return y\n",
    "\n",
    "\n",
    "def load_train(load_repr=load_mel):\n",
    "    '''Wczytuje dane treningowe.\n",
    "    \n",
    "    Args:\n",
    "        load_repr (function): Funkcja wczytująca pożądaną reprezentację.\n",
    "    \n",
    "    Returns:\n",
    "        (ndarray, ndarray): Tablica z danymi treningowymi, tablica z binarnymi etykietami treningowymi.\n",
    "    '''\n",
    "    labels = read_labels()\n",
    "    X_train, y_train = [], []\n",
    "    rec_files = [file_name for file_name in os.listdir('train') if file_name.endswith('.wav')]\n",
    "    print(rec_files)\n",
    "    for file_name in rec_files:\n",
    "        recording_id = int(file_name.split('.')[0][3:])\n",
    "        recording_labels = labels[labels[:, 0] == recording_id]\n",
    "        y_binary = map_seconds_to_y(recording_labels)\n",
    "        for i, y in enumerate(y_binary):\n",
    "            if y != -1:\n",
    "                try:\n",
    "                    representation = load_repr(os.path.join('train', file_name), start = i, stop = i + 1)\n",
    "                    X_train.append(representation)\n",
    "                    y_train.append(y)\n",
    "                except ValueError:\n",
    "                    print('Error reading file', file_name)\n",
    "                except TypeError:\n",
    "                    print('Unsupported type', file_name)\n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "def prepare_training_set(labels, interval):\n",
    "    starts, stops, y_binary = [], [], []\n",
    "    compartment = interval / 2\n",
    "    # creating positive samples \n",
    "    for label in labels:\n",
    "        middle = label[1] + (label[2] - label[1]) / 2 \n",
    "        start = middle - compartment\n",
    "        starts.append(start)\n",
    "        stop = middle + compartment\n",
    "        stops.append(stop)\n",
    "        if start < 0 or stop > 10:\n",
    "            y_binary.append(-1)\n",
    "        else: \n",
    "            y_binary.append(1)\n",
    "    # creating negative samples\n",
    "    loop_counter = len(labels) \n",
    "    while loop_counter:\n",
    "        middle = torch.FloatTensor(1).uniform_(compartment, 10 - compartment).numpy()\n",
    "        start = middle - compartment \n",
    "        stop = middle + compartment\n",
    "        check = np.zeros(len(starts))\n",
    "        for i in range(len(starts)):\n",
    "            if start > stops[i] or stop < starts[i]:\n",
    "                check[i] = 1\n",
    "            else:\n",
    "                check[i] = 0\n",
    "        if np.all(check):\n",
    "            loop_counter -= 1\n",
    "            starts.append(start)\n",
    "            stops.append(stop)\n",
    "            y_binary.append(0)\n",
    "            \n",
    "    return starts, stops, y_binary\n",
    "\n",
    "def load_train_2(load_repr=load_mel):\n",
    "    '''Wczytuje dane treningowe.\n",
    "    \n",
    "    Args:\n",
    "        load_repr (function): Funkcja wczytująca pożądaną reprezentację.\n",
    "    \n",
    "    Returns:\n",
    "        (ndarray, ndarray): Tablica z danymi treningowymi, tablica z binarnymi etykietami treningowymi.\n",
    "    '''\n",
    "    labels = read_labels()\n",
    "    X_train, y_train = [], []\n",
    "    rec_files = [file_name for file_name in os.listdir('train') if file_name.endswith('.wav')]\n",
    "    for file_name in tqdm(rec_files):\n",
    "        recording_id = int(file_name.split('.')[0][3:])\n",
    "        recording_labels = labels[labels[:, 0] == recording_id]\n",
    "#         print('recording labels:', recording_labels)\n",
    "        starts, stops, y_binary = prepare_training_set(recording_labels, 0.3)\n",
    "#         print(\"starts:\", starts)\n",
    "#         print(\"stops:\", stops)\n",
    "        assert len(starts) == len(stops) == len(y_binary)\n",
    "        for start, stop, y in zip(starts, stops, y_binary):\n",
    "            if y != -1:\n",
    "                try:\n",
    "    #                 print('start:',start)\n",
    "    #                 print('stop:',stop)\n",
    "    #                 print('filename:',file_name)\n",
    "                    representation = load_repr(os.path.join('train', file_name), start=start, stop=stop)\n",
    "                    X_train.append(representation)\n",
    "                    y_train.append(y)\n",
    "    #                 print(X_train[0].shape)\n",
    "    #                 print(y_train[0])\n",
    "                except ValueError as e:\n",
    "                    print('Error reading file', file_name)\n",
    "                    print(e)\n",
    "                except TypeError as e:\n",
    "                    print('Unsupported type', file_name)\n",
    "                    print(e)\n",
    "    return np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-1].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-1].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append([])\n",
    "a[-1].append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a[-1].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0.15,1,0.1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zapisywanie Wczytanej Reprezentacji\n",
    "\n",
    "Ponieważ tworzenie reprezentacji może zabierać sporo czasu (szczególnie w tak naiwnej implementacji jak powyższa), warto zapisać wczytane dane do plików."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import librosa.core.logamplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "spec = load_mel(os.path.join(os.getcwd(), 'train','rec25.wav'),start=9.1, stop=9.4)\n",
    "# spec = librosa.power_to_db(spec,ref=5.0)\n",
    "librosa.display.specshow(spec, y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(S=spec, norm='ortho', dct_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Poniższa linijka ustawia folder główny\n",
    "# os.chdir('../input/')\n",
    "\n",
    "X_test, recordings_ids = load_test_2(interval=0.3, step=0.05)\n",
    "# np.save(os.path.join('test', 'tmp_X_test'), X_test)\n",
    "\n",
    "X, y = load_train_2()\n",
    "# np.save(os.path.join('train', 'tmp_X_train'), X)\n",
    "# np.save(os.path.join('train', 'tmp_y_train'), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_x_64 = X_test[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_x = X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_native = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_native.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "librosa.display.specshow(X_test[1000], y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "spec = load_mel(os.path.join(os.getcwd(), 'test','rec8.wav'),start=0, stop=10, n_mels=60)\n",
    "# spec = librosa.power_to_db(spec,ref=5.0)\n",
    "librosa.display.specshow(spec, y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('test', 'tmp_X_test'), X_test)\n",
    "np.save(os.path.join('train', 'tmp_X_train'), X)\n",
    "np.save(os.path.join('train', 'tmp_y_train'), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(os.path.join(os.getcwd(), 'test', 'tmp_X_test.npy'))\n",
    "X = np.load(os.path.join(os.getcwd(), 'train', 'tmp_X_train.npy'))\n",
    "y = np.load(os.path.join(os.getcwd(), 'train', 'tmp_y_train.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(X[100], y_axis='mel', x_axis='time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model i Trenowanie\n",
    "\n",
    "Poniższy przykład używa poprawnych metryk i zapisuje parametry modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Detector(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.linear = torch.nn.Linear(10 * 87, 2)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = torch.flatten(x, start_dim = 1)\n",
    "#         out = self.linear(out)\n",
    "#         return out\n",
    "    \n",
    "# clf = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = np.load(os.path.join(os.getcwd(), 'test', 'tmp_X_test.npy'))\n",
    "X = np.load(os.path.join(os.getcwd(), 'train', 'tmp_X_train.npy'))\n",
    "y = np.load(os.path.join(os.getcwd(), 'train', 'tmp_y_train.npy'))\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(52)\n",
    "class ConvNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, block):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # wstępna warstwa konwolucyjna + ReLU\n",
    "        self.conv = torch.nn.Conv2d(1, 8, kernel_size=(1,20), padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batch_norm1 = torch.nn.BatchNorm2d(8)\n",
    "        \n",
    "        # (mikroarchitektura + max pooling) x 2\n",
    "#         self.block1 = block(16, 32)\n",
    "#         self.pool1 = torch.nn.MaxPool2d((1,10))\n",
    "#         self.block2 = block(32, 64)\n",
    "        self.pool2 = torch.nn.MaxPool2d((1,60))\n",
    "        \n",
    "        # warstwa w pełni połączona po \"rozprostowaniu\" obrazu do postaci wektora\n",
    "        self.dense = torch.nn.Linear(496, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        log = False \n",
    "        if log:\n",
    "            print(x.shape)\n",
    "        out = self.relu(self.batch_norm1(self.conv(x)))\n",
    "        if log:\n",
    "            print(out.shape)\n",
    "#         out = self.block1(out)\n",
    "#         if log:\n",
    "#             print(out.shape)\n",
    "#         out = self.pool1(out)\n",
    "#         if log:\n",
    "#             print(out.shape)\n",
    "#         out = self.block2(out)\n",
    "#         if log:\n",
    "#             print(out.shape)\n",
    "        out = self.pool2(out)\n",
    "        if log:\n",
    "            print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        if log:\n",
    "            print(out.shape)\n",
    "        return self.dense(out)\n",
    "    \n",
    "class BaseBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=None):\n",
    "        super(BaseBlock, self).__init__()\n",
    "        if not hidden_channels:\n",
    "            hidden_channels = out_channels\n",
    "\n",
    "        # konwolucja zawiera padding=1, aby nie zmniejszać rozmiaru obrazu\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, hidden_channels, kernel_size=(1,20) )\n",
    "        # istnieje wersja batch normalization dla obrazów - statystyki dla filtrów\n",
    "        self.batch_norm1 = torch.nn.BatchNorm2d(hidden_channels)\n",
    "        # nieliniowość ReLU\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "\n",
    "        # drugi raz te same warstwy\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, out_channels, kernel_size=(1,20))\n",
    "        self.batch_norm2 = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.batch_norm1(self.conv1(x)))\n",
    "        out = self.relu2(self.batch_norm2(self.conv2(out)))\n",
    "        return out   \n",
    "\n",
    "# class ResNetBlock(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, in_channels, out_channels, hidden_channels=None):\n",
    "#         super(ResNetBlock, self).__init__()\n",
    "#         if not hidden_channels:\n",
    "#             hidden_channels = out_channels\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.conv1 = torch.nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "#         self.batch_norm1 = torch.nn.BatchNorm2d(hidden_channels) \n",
    "#         self.relu1 = torch.nn.ReLU()\n",
    "#         self.conv2 = torch.nn.Conv2d(hidden_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.conv3 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.batch_norm2 = torch.nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.relu1(self.batch_norm1(self.conv1(x)))\n",
    "#         out = self.conv2(out)\n",
    "#         if self.in_channels < self.out_channels:\n",
    "#             x = self.conv3(x)\n",
    "#         out = torch.add(x, out)\n",
    "#         out = self.relu2(self.batch_norm2(out))\n",
    "#         return out\n",
    "from torch import nn\n",
    "class AlexNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "clf = ConvNet(BaseBlock)\n",
    "# clf = AlexNet()\n",
    "\n",
    "# Dzielenie zbioru danych na treningowy i walidacyjny\n",
    "split_point = int(len(X) * 0.9)\n",
    "\n",
    "X_train = torch.Tensor(X[:split_point]).unsqueeze(1)\n",
    "y_train = torch.LongTensor(y[:split_point])\n",
    "\n",
    "X_valid = torch.Tensor(X[split_point:]).unsqueeze(1)\n",
    "y_valid = torch.LongTensor(y[split_point:])\n",
    "\n",
    "batch_size = 16 \n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "data_loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size = batch_size)\n",
    "\n",
    "# Ustawienie kosztu i optimizera\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters())\n",
    "\n",
    "# Pętla uczenia\n",
    "best_preds, best_score = None, 0.\n",
    "losses, scores = [], []\n",
    "epochs_number = 10 \n",
    "for epoch in trange(epochs_number):\n",
    "    running_loss = 0\n",
    "    clf.train()\n",
    "    for X, y in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = clf(X)\n",
    "#         print('outputs: ', outputs)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    losses.append(running_loss)\n",
    "    \n",
    "    clf.eval()\n",
    "    preds = []\n",
    "    for X, _ in valid_data_loader:\n",
    "        out = clf(X)\n",
    "        preds.append(torch.softmax(out, dim = 1)[:, 1].detach().numpy())\n",
    "    preds = np.concatenate(preds, axis = 0)\n",
    "    \n",
    "    # Metryką testującą jest ROC AUC\n",
    "    score = roc_auc_score(y_valid.numpy(), preds)\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_preds = preds\n",
    "        np.save('tmp_preds', best_preds)\n",
    "        # Model dający najlepszy wynik powinien być zapisany\n",
    "        torch.save(clf.state_dict(), 'tmp_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rysowanie lossu i AUC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zapis Predykcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(preds):\n",
    "    '''Zapisuje predykcje do pliku zgodnego z formatem odpowiedzi.\n",
    "    \n",
    "    Args:\n",
    "        preds (list): Lista predykcji (prawdopodobieństw).\n",
    "    '''\n",
    "    with open('sampleSubmission.csv', 'r') as file:\n",
    "        submission_text = file.read().split()\n",
    "        header = submission_text[0]\n",
    "        lines = submission_text[1:]\n",
    "\n",
    "    output_lines = [header]\n",
    "    for pred, line in zip(preds, lines):\n",
    "        output_lines.append(\"{},{}\".format(line.split(',')[0], pred))\n",
    "    \n",
    "    with open('submission.csv', 'w') as file:\n",
    "        file.write('\\n'.join(output_lines) + '\\n')\n",
    "        \n",
    "def save_predictions_2(preds: dict):\n",
    "    '''Zapisuje predykcje do pliku zgodnego z formatem odpowiedzi.\n",
    "    \n",
    "    Args:\n",
    "        preds (list): Lista predykcji (prawdopodobieństw).\n",
    "    '''\n",
    "    with open('sampleSubmission.csv', 'r') as file:\n",
    "        submission_text = file.read().split()\n",
    "        header = submission_text[0]\n",
    "        lines = submission_text[1:]\n",
    "\n",
    "    output_lines = [header]\n",
    "    for line in lines:\n",
    "        print(line.split(',')[0])\n",
    "        indx = line.split(',')[0]\n",
    "        output_lines.append(\"{},{}\".format(indx, preds[indx]))\n",
    "    \n",
    "    with open('submission.csv', 'w') as file:\n",
    "        file.write('\\n'.join(output_lines) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie najlepszego modelu\n",
    "from collections import defaultdict\n",
    "clf.load_state_dict(torch.load('tmp_model.pt'))\n",
    "\n",
    "# Tworzenie data loadera testowego\n",
    "X_test_tensor = torch.Tensor(X_test).unsqueeze(1)\n",
    "print(X_test_tensor.shape)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "# Ewaluacja modelu na danych testowych\n",
    "#TODO - create a threshold?\n",
    "clf.eval()\n",
    "preds = []\n",
    "for i, X in enumerate(test_data_loader):\n",
    "    if i == 0:\n",
    "        print(type(X[0][0]))\n",
    "        assert torch.all(torch.eq(X[0][0], torch.tensor(assert_x, dtype=torch.float)))\n",
    "        print(recordings_ids[0])\n",
    "#     if i == 4:\n",
    "#         assert torch.all(torch.eq(X[0][2], torch.tensor(assert_x_64, dtype=torch.float)))\n",
    "#         print(recordings_ids[64])\n",
    "        \n",
    "    \n",
    "    out = clf(X[0])\n",
    "    print('out :',out)\n",
    "    \n",
    "    preds.append(torch.softmax(out, dim = 1)[:, 1].detach().numpy())\n",
    "    print(preds)\n",
    "#     for j, pred in enumerate(preds):\n",
    "#         try:\n",
    "#             prediction_dict[int(recordings_ids[i * batch_size + j][0])].append(preds[i][j])\n",
    "#         except:\n",
    "#             print('end')\n",
    "    \n",
    "#     print(preds[0].shape)\n",
    "#     print(preds[0])\n",
    "    \n",
    "preds = np.concatenate(preds, axis = 0)\n",
    "print(preds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = preds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_std = preds.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = defaultdict(list) \n",
    "for i, pred in enumerate(preds):\n",
    "    prediction_dict[str(recordings_ids[i])].append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging\n",
    "final_preds_dict= defaultdict(float) \n",
    "\n",
    "for k in prediction_dict.keys():\n",
    "    v = prediction_dict[k] - global_mean\n",
    "    final_preds_dict[k] = (np.maximum(v, 0, v  ) / mv).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only max value\n",
    "final_preds_dict= defaultdict(float) \n",
    "\n",
    "for k in prediction_dict.keys():\n",
    "    final_preds_dict[k] = np.array(prediction_dict[k]).max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds_dict['39/6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import librosa.display\n",
    "spec = load_mel(os.path.join(os.getcwd(), 'test','rec39.wav'),start=0, stop=10, n_mels=60)\n",
    "# spec = librosa.power_to_db(spec,ref=5.0)\n",
    "librosa.display.specshow(spec, y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import librosa.display\n",
    "spec = load_mel(os.path.join(os.getcwd(), 'test','rec76.wav'),start=4, stop=5.0, n_mels=60)\n",
    "# spec = librosa.power_to_db(spec,ref=5.0)\n",
    "librosa.display.specshow(spec, y_axis='mel', x_axis='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Zapisanie predykcji do poprawnego formatu\n",
    "save_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions_2(final_preds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds_dict['1/1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
